{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision tree example using IG (Info Gain)\n",
    "# Source: https://github.com/thushv89/exercises_thushv_dot_com/blob/master/decision_trees_light_on_math_ml/decision_trees.ipynb\n",
    "# For additional decision tree code pls refer https://github.com/random-forests/tutorials/blob/master/decision_tree.ipynb\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Outlook</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Windy</th>\n",
       "      <th>Play_Golf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Outlook  Temperature  Humidity  Windy  Play_Golf\n",
       "0        0            2         1      0          0\n",
       "1        0            2         1      1          0\n",
       "2        1            2         1      0          1\n",
       "3        2            1         1      0          1\n",
       "4        2            0         0      0          1\n",
       "5        2            0         0      1          0\n",
       "6        1            0         0      1          1\n",
       "7        0            1         1      0          0\n",
       "8        2            1         0      1          0\n",
       "9        2            1         0      0          1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an example dataset of weather data and its relationship with playing golf.\n",
    "# Sample (toy_data) is of categorical variables denoted by letters\n",
    "# Data for 10 days with Target variable \"Play_Golf\" being Yes or No\n",
    "# The four Weather variables are 1. Outlook (Rainy, Sunny, Overcast), 2. Temperature (Hot, Medium, Cold)\n",
    "# 3. Humidity (High, Normal), 4. Windy (Ture, False)\n",
    "data = pd.DataFrame(\n",
    "{\"Outlook\":     [\"R\", \"R\", \"O\", \"S\", \"S\", \"S\", \"O\", \"R\", \"S\", \"S\"],\n",
    " \"Temperature\": [\"H\", \"H\", \"H\", \"M\", \"C\", \"C\", \"C\", \"M\", \"M\", \"M\"],\n",
    " \"Humidity\":    [\"H\", \"H\", \"H\", \"H\", \"N\", \"N\", \"N\", \"H\", \"N\", \"N\"],\n",
    " \"Windy\":       [\"F\", \"T\", \"F\", \"F\", \"F\", \"T\", \"T\", \"F\", \"T\", \"F\"],\n",
    " \"Play_Golf\":   [\"N\", \"N\", \"Y\", \"Y\", \"Y\", \"N\", \"Y\", \"N\", \"N\", \"Y\"]\n",
    "}\n",
    ")\n",
    "\n",
    "# Mapping the letters in the dataframe to numbers\n",
    "data[\"Outlook\"] = data[\"Outlook\"].map({\"R\":0, \"O\": 1, \"S\": 2})\n",
    "data[\"Temperature\"] = data[\"Temperature\"].map({\"C\":0, \"M\": 1, \"H\": 2})\n",
    "data[\"Humidity\"] = data[\"Humidity\"].map({\"N\":0, \"H\": 1})\n",
    "data[\"Windy\"] = data[\"Windy\"].map({\"F\":0, \"T\": 1})\n",
    "data[\"Play_Golf\"] = data[\"Play_Golf\"].map({\"N\":0, \"Y\": 1})\n",
    "\n",
    "data.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' For this decision tree create a function to calcluate information gain. \n",
    "Manually compute information gain for different features.\n",
    "We validate our computation by computing information gain for two edge cases (i.e. a pure split, random split). \n",
    "We will see that a pure split gives an information gain=1 and random split gives information gain=0.'''\n",
    "\n",
    "def compute_infogain(data, feature, label_col_name):\n",
    "    \"\"\"\n",
    "    This function computes the information gain\n",
    "    IG(Y|X) = H(Y) - H(Y|X)\n",
    "    data: A pd.DataFrame container all data\n",
    "    grouped_data: A groupby object that groups Play_GOLF value counts by some feature\n",
    "    \"\"\"\n",
    "\n",
    "    grouped_data = data.groupby(feature)[label_col_name].value_counts()\n",
    "    \n",
    "    h_y = compute_entropy(data[label_col_name])\n",
    "    h_y_given_x = 0\n",
    "    #print(grouped_data)\n",
    "    for k in grouped_data.keys():\n",
    "        # k is a tuple, which has the feature index followed by label index (groupby object)\n",
    "        k_f, k_y = k\n",
    "        h_y_given_x += (grouped_data[k_f][k_y].sum()*1.0/data.shape[0])* compute_entropy_with_counts(grouped_data[k_f])\n",
    "    \n",
    "    return h_y - h_y_given_x\n",
    "    \n",
    "def compute_entropy(ser):\n",
    "    \"\"\"\n",
    "    This function computes the entropy\n",
    "    \"\"\"\n",
    "    total = 0\n",
    "    counts = ser.value_counts()\n",
    "    for k in ser.unique():\n",
    "        #print(k)\n",
    "        total += -(counts[k]*1.0/ser.shape[0])*np.log2(counts[k]*1.0/ser.shape[0])\n",
    "        \n",
    "    return total\n",
    "\n",
    "def compute_entropy_with_counts(data_counts):\n",
    "    \"\"\"\n",
    "    This function computes the entropy\n",
    "    \"\"\"\n",
    "    total = 0\n",
    "    for k in data_counts:\n",
    "        total += -(k*1.0/data_counts.sum())*np.log2(k*1.0/data_counts.sum())\n",
    "       \n",
    "    return total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking information gain with toy data\n",
      "\tInformation Gain for Toy data Pure Split is: Actual (1.0) Expected (1.0)\n",
      "\tInformation Gain for Toy data Random Split is: Actual (0.0) Expected (0.0)\n",
      "\n",
      "Checking information gain with actual data\n",
      "\tEntropy for Play_Golf is: 1.0\n",
      "\n",
      "\tInformation gain for feature Outlook is : 0.5145247027726657\n",
      "\tInformation gain for feature Temperature is : 0.0490224995673062\n",
      "\tInformation gain for feature Humidity is : 0.02904940554533142\n",
      "\tInformation gain for feature Windy is : 0.12451124978365313\n"
     ]
    }
   ],
   "source": [
    "## Making sure computations are correct\n",
    "\n",
    "## The answer should be 1.0 because the data will be perfectly split to 0s and 1s\n",
    "## by splitting according to x\n",
    "print('Checking information gain with toy data')\n",
    "toy_data = pd.DataFrame({'x':[0,0,0,0,1,1,1,1], 'y':[0,0,0,0,1,1,1,1]})\n",
    "print('\\tInformation Gain for Toy data Pure Split is: Actual ({}) Expected ({})'.format(\n",
    "    compute_infogain(toy_data, 'x', \"y\"), 1.0\n",
    "))\n",
    "\n",
    "## The answer should be 0.0 because the data will be random after splitting according to x\n",
    "toy_data = pd.DataFrame({'x':[1,0,1,0,1,0,1,0], 'y':[0,0,0,0,1,1,1,1]})\n",
    "print('\\tInformation Gain for Toy data Random Split is: Actual ({}) Expected ({})\\n'.format(\n",
    "    compute_infogain(toy_data, 'x', \"y\"), 0.0\n",
    "))\n",
    "\n",
    "print('Checking information gain with actual data')\n",
    "print('\\tEntropy for Play_Golf is: {}\\n'.format(compute_entropy(data[\"Play_Golf\"])))\n",
    "\n",
    "for col in [\"Outlook\", \"Temperature\", \"Humidity\", \"Windy\"]:\n",
    "    print('\\tInformation gain for feature {} is : {}'.format(col, compute_infogain(data, col, \"Play_Golf\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Computing the tree. Create a decision tree, which uses information gain to find the best split. \n",
    "The algorithm is as follows:\n",
    "1. Find a feature to split data (based on the information gain)\n",
    "2. Split/Partition data into n sets depending on the unique values\n",
    "   For each partition, if termination condition is not met\n",
    "   a. Repeat step 1\n",
    "   b. Repeat step 2\n",
    "Note : This tree model cannot work with continuous features, but only discrete/categorical features.'''\n",
    "\n",
    "# Create Tree Objects ie. Decision Nodes and Leaf Nodes\n",
    "\n",
    "class DecisionNode:\n",
    "\n",
    "    def __init__(self, feature, value, data=None, children=None):\n",
    "        self.feature = feature\n",
    "        self.value = value\n",
    "        self.data = data\n",
    "        self.children = children\n",
    "\n",
    "    def set_parent(self, parent):\n",
    "        self.parent = parent\n",
    "\n",
    "    def set_data(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "class LeafNode:\n",
    "    \n",
    "    def __init__(self, data, label_column):\n",
    "        self.prediction = data[label_column].value_counts().idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Tree related helper functions ie. partitioning and finding best feature to partition\n",
    "def partition_data(feature, values, data_to_split):\n",
    "    \"\"\"\n",
    "    Splits the data such that each dataframe in the returned list \n",
    "    has the same value for the selected feature\n",
    "    \"\"\"\n",
    "    data_splits = {}\n",
    "    for v in values:\n",
    "        true_data = data_to_split.loc[data_to_split[feature]==v,:]\n",
    "        data_splits[v] = true_data\n",
    "    return data_splits\n",
    "\n",
    "\n",
    "def find_best_feature(data_to_split, features, label_column):\n",
    "    \"\"\"\n",
    "    This function finds the best feature to split that maximizes\n",
    "    the information gain for a given dataframe\n",
    "    \"\"\"\n",
    "    #feature_infogain_tuples = []\n",
    "    max_feature, max_ig = None, -1e10\n",
    "    for f in features:\n",
    "        #partitions = partition_data(f, set(data_to_split[f].tolist()), data_to_split)\n",
    "        ig = compute_infogain(data_to_split, f, label_column)\n",
    "        if ig >= max_ig:\n",
    "            max_feature = f\n",
    "            max_ig = ig\n",
    "    return max_feature, max_ig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Recursive tree building method\n",
    "In this build_tree function, I use two termination conditions to terminate the growth of a branch\n",
    "1. The number of datapoints in a branch is less than min_leaf_count\n",
    "2. The information gained by splitting is less than ig_tol\n",
    "3. The depth of a branch is greater than max_depth'''\n",
    "def build_tree(data_to_split, label_column, features, best_feature=None, value=None, depth=0, min_leaf_count=3, ig_tol=1e-10, max_depth=5):\n",
    "    \"\"\"\n",
    "    This Function computes the sub tree recursively. This is a more general tree model\n",
    "    where there can be arbitrary number of children for a given node\n",
    "    :param data_to_split: pd.DataFrame\n",
    "    :param label_column: str (Column name of Y)\n",
    "    :param features: list of str (Column names of X)\n",
    "    :param best_feature: str (previous best feature fed recursively to build the tree)\n",
    "    :param value: int (previous value of the feature fed recursively)\n",
    "    :param depth: int (previous depth of the tree fed recursively)\n",
    "    :param min_leaf_count: int (minimum number of datapoints in a leaf)\n",
    "    :param ig_tol: float (information gain tolerance to make a split)\n",
    "    :param max_depth: int (maximum depth allowed in the tree)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sanity check (ig_tol needs to be > 0). Otherwise\n",
    "    # data having the same label before and after the split \n",
    "    # (information gain =0) will create many useless branches\n",
    "    if ig_tol<=0.0:\n",
    "        print('ig_tol needs to be > 0')\n",
    "        return\n",
    "    \n",
    "    # Termination condition 1 (minimum count on leaf)\n",
    "    if data_to_split.shape[0]<=min_leaf_count:\n",
    "        print('Too little data. Terminating growth ...')\n",
    "        children = [LeafNode(data_to_split, label_column)]\n",
    "        return DecisionNode(best_feature, value, data_to_split, children=children)\n",
    "    \n",
    "    # Finding the next best feature to split the data on\n",
    "    next_best_feature, infogain = find_best_feature(data_to_split, features, label_column)\n",
    "    feature_unique_values = list(set(data_to_split[next_best_feature].tolist()))\n",
    "    \n",
    "    # Termination condition 2 (minimum information gain)\n",
    "    if infogain < ig_tol or next_best_feature is None:\n",
    "        print('Too little information gain. Terminating growth ...')\n",
    "        children = [LeafNode(data_to_split, label_column)]\n",
    "        return DecisionNode(best_feature, value, data_to_split, children=children)\n",
    "    \n",
    "    next_depth = depth + 1\n",
    "    \n",
    "    # Termination condition 3 (depth)\n",
    "    if depth >= max_depth:\n",
    "        print('Too deep. Terminating growth ...')\n",
    "        children = [LeafNode(data_to_split, label_column)]\n",
    "        return DecisionNode(best_feature, value, data_to_split, children=children)\n",
    "    \n",
    "    print('Choosing {} as the best feature with {} information gain at depth {}'.format(next_best_feature, infogain, next_depth))\n",
    "    # Partition the data according to the selected features values\n",
    "    parts_dict = partition_data(next_best_feature, feature_unique_values, data_to_split)\n",
    "    \n",
    "    # For each partition create a child, where child recursively calls build_tree\n",
    "    children = []\n",
    "    \n",
    "    for attr, p in parts_dict.items():\n",
    "        print('\\tCreating child node {}={} having {} data points...'.format(next_best_feature, attr, p.shape[0]))\n",
    "        children.append(build_tree(p, label_column, features, next_best_feature, attr, next_depth, min_leaf_count, ig_tol, max_depth))\n",
    "        \n",
    "    # Return the node\n",
    "    return DecisionNode(best_feature, value, data_to_split, children=children)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choosing Outlook as the best feature with 0.5145247027726657 information gain at depth 1\n",
      "\tCreating child node Outlook=0 having 3 data points...\n",
      "Too little data. Terminating growth ...\n",
      "\tCreating child node Outlook=1 having 2 data points...\n",
      "Too little data. Terminating growth ...\n",
      "\tCreating child node Outlook=2 having 5 data points...\n",
      "Choosing Windy as the best feature with 0.9709505944546686 information gain at depth 2\n",
      "\tCreating child node Windy=0 having 3 data points...\n",
      "Too little data. Terminating growth ...\n",
      "\tCreating child node Windy=1 having 2 data points...\n",
      "Too little data. Terminating growth ...\n"
     ]
    }
   ],
   "source": [
    "# Running, building the tree\n",
    "# Returns the root node\n",
    "my_tree = build_tree(data, \"Play_Golf\", [\"Outlook\", \"Temperature\", \"Humidity\", \"Windy\"], ig_tol=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing the decision tree ...\n",
      "  Root\n",
      "   Node Outlook = 0\n",
      "    Leaf Prediction:  0\n",
      "   Node Outlook = 1\n",
      "    Leaf Prediction:  1\n",
      "   Node Outlook = 2\n",
      "    Node Windy = 0\n",
      "     Leaf Prediction:  1\n",
      "    Node Windy = 1\n",
      "     Leaf Prediction:  0\n"
     ]
    }
   ],
   "source": [
    "# Printing the tree\n",
    "print('Printing the decision tree ...')\n",
    "def print_tree(node, spacing=''):\n",
    "    \"\"\"\n",
    "    This function recursively prints the tree with indentation\n",
    "    \"\"\"\n",
    "    spacing += ' '\n",
    "    \n",
    "    if isinstance(node, LeafNode):\n",
    "        print(spacing, 'Leaf Prediction: ', node.prediction)\n",
    "        return \n",
    "    \n",
    "    if node.feature==None and node.value==None:\n",
    "        print(spacing, \"Root\")\n",
    "    else:\n",
    "        print(spacing, \"Node\",node.feature, '=', node.value)\n",
    "        \n",
    "    for c in node.children:\n",
    "        print_tree(c, spacing)\n",
    "\n",
    "\n",
    "print_tree(my_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting with the tree\n",
    "def predict_datapoint(test_point, my_tree):\n",
    "    \"\"\"\n",
    "    This function computes the prediction for a given data point\n",
    "    \"\"\"\n",
    "    \n",
    "    for c in my_tree.children:\n",
    "        # If we came to a leaf node, return the prediction given by the leaf node\n",
    "        if isinstance(c, LeafNode):\n",
    "            return c.prediction\n",
    "        \n",
    "        # If the feature value matches the value of the data point, keep diving\n",
    "        if test_point[c.feature] == c.value:\n",
    "            return predict_datapoint(test_point, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label for {'Outlook': 0, 'Humidity': 0, 'Temperature': 2, 'Windy': 0}\n",
      "\tPredicted label (Play_Golf): False\n"
     ]
    }
   ],
   "source": [
    "datapoint = {\"Outlook\": 0, \"Humidity\": 0, \"Temperature\": 2, \"Windy\": 0}\n",
    "print('Predicted label for {}'.format(datapoint))\n",
    "\n",
    "pred = predict_datapoint(datapoint, my_tree)\n",
    "print('\\tPredicted label (Play_Golf): {}'.format(False if pred==0 else True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
